# Homework 9: Distributed Training and Neural Machine Translation


## Objective
Using Nvidia [OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq/) framework for sequence to sequence tasks to train a [Transformer-based Machine Translation network](https://nvidia.github.io/OpenSeq2Seq/html/machine-translation/transformer.html) on a small English to German WMT corpus. To do this, we will get a pair of IBM VSI instances with 2 V100 GPUs each.

## Setup

### Creating cloud containers
I creatd two IBM cloud VMs, each with 2 V100 GPUs

```
ibmcloud sl vs create --datacenter=lon04 --hostname=v100a --domain=apik.com --image=2263543 --billing=hourly  --network 1000 --key=<my_key> --flavor AC2_16X120X100 â€“san

ibmcloud sl vs create --datacenter=lon04 --hostname=v100b --domain=apik.com --image=2263543 --billing=hourly  --network 1000 --key=<my_key> --flavor AC2_16X120X100 --san
```
I also added 2 TB of storage on each VM, to make sure they would be able to hold the data and models we would be generating. After starting each of the VMs, I mounted these disks using the following steps:

```
mkdir -m 777 /data
mkfs.ext4 /dev/xvdc
# edit /etc/fstab and all this line: /dev/xvdc /data                   ext4    defaults,noatime        0 0

# Mount
mount /data
```

After creating an account on https://ngc.nvidia.com/ , I logged in on both of my machines and pulled tensorflow

```
docker login nvcr.io
#Username: $oauthtoken
#Password: API KEY
docker pull nvcr.io/nvidia/tensorflow:19.05-py3
```

## Distributed Training
I cloned our repo and build the `openseq2seq1` image and then created a container on both VMs

```
git clone https://github.com/MIDS-scaling-up/v2
cd v2/week09/hw/docker/
docker build -t openseq2seq .

docker run --runtime=nvidia -d --name openseq2seq --net=host -e SSH_PORT=4444 -v /data:/data -p 6006:6006 openseq2seq
```

On each VM, i created an interactive bash session and downloaded the data for training
```
docker exec -ti openseq2seq bash
mpirun -n 2 -H <v100a_private_ip>,<v100b_private_ip>--allow-run-as-root hostname
cd ../opt/OpenSeq2Seq 
scripts/get_en_de.sh /data/wmt16_de_en
cp example_configs/text2text/en-de/transformer-base.py /data
```

Finally, i edited the config file with the following changes:
1. Replace [REPLACE THIS TO THE PATH WITH YOUR WMT DATA] with /data/wmt16_de_en/
1. In base_parms section, replace "logdir": "nmt-small-en-de", with "logdir": "/data/en-de-transformer/", 
1. In eval_params section set "repeat": to True.
1. Modify the config file to use mixed precision per the instructions in the file


I also changed the step limit to 50,000, so as to avoid running out of credits. Once these changes were made, I ran the following on the `v100a` instance:

```
nohup mpirun --allow-run-as-root -n 4 -H <v100a_private_ip>:2,<v100b_private_ip>:2 -bind-to none -map-by slot --mca btl_tcp_if_include eth0 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH python run.py --config_file=/data/transformer-base.py --use_horovod=True --mode=train_eval &
```



## Setbacks while running 
During training, I was unable to start tensorboard from my instance, so the only way I was able to monitor my training was by monitoring the nohup.out file.

Leading up to my succesful training, I had many issues reaching the training stage which took several days. First, I was unable to get two machines with 2 V100 GPUs at the same time. I would start my machines and run `nvidia-smi` each time, but I was constantly faced with the issue where only one of them would have a GPU, or I would get an error running `nvidia-smi`. The eventual solution was to change my machine's location from was04 to lon04

Another issue I had was that I would reach the training stage and and right after running the `nohip mpirun` step, I would receive a message that said `nohup: ignoring input and appending output to nohup.out` I turned out that this could be solved by removing a space I had after the comma separating `<v100a_private_ip>:2,<v100b_private_ip>:2`. Luckily, I was informed via slack by another student of this fix.

## Results

Below are some images from dockerhub. I saved my `/en-de-transformer` directory and copied it to my local machine, from where I was able to start tensorboard to capture these plots:

BLUE Score
![Validation BLEU curve](https://i.ibb.co/g42hJQp/bleu.jpg)

Validation Loss Curve
![Validation loss curve](https://i.ibb.co/JdHBvzZ/eval-loss.jpg)

Learning Rate Curve
![Learning rate curve](https://i.ibb.co/TmrbKGs/learn-rate.jpg)

Training Loss Curve
![Training loss curve](https://i.ibb.co/T1v9WjT/train-loss.jpg)

Loss Opt
![Loss Opt](https://i.ibb.co/0qSzrQY/loss-opt.jpg)

* How long did it take to complete the training run? (hint: this session is on distributed training, so it *will* take a while)
Training took 23 hours and 23 minutes

* Do you think your model is fully trained? How can you tell?
I do not think my model is fully trained, because we can see that the BLEU graph is still progressing and the loss is moving towards, but may not have yet reached a minimum. Also, I stopped my training at 50,000 steps, which may have been too soon.

* Were you overfitting?
I believe we were beginning to overfit because after 30K steps, we can see that the evaluation loss is flattening out, which leads me to believe that we are overfitting the model.

* Were your GPUs fully utilized?
I would randomly check the GPU utilization and saw that one of my two machines always had both GPUs at 100%, but the other's GPUs would be mostly utilized but not at 100%:

V100b's GPUs utilized at less than 100%, V100b's GPUs utilized fully at 100% each
![GPU Util](https://i.ibb.co/MhTX2yM/Screen-Shot-2020-03-07-at-12-40-46-PM.jpg)

An hour later, V100a's GPUs utilized at 100%, V100b's GPUs at less than 100%
![GPU Util](https://i.ibb.co/4jsMk88/Screen-Shot-2020-03-07-at-1-33-31-PM.jpg)


* Did you monitor network traffic (hint:  ```apt install nmon ```) ? Was network the bottleneck?
No, the network was not the bottleneck

* Take a look at the plot of the learning rate and then check the config file.  Can you explan this setting?
The learning rate demonstartes the size of the step that the model will take while learning. In the LSTM, the learning rate correlates to the context level at each step that the model learns from. We see that as our steps increase, the learning rate decreases. The model has progressively learned more, and it takes smaller steps to finetune its learning and improve backpropagation.

* How big was your training set (mb)? How many training lines did it contain?


* What are the files that a TF checkpoint is comprised of?
The TF checkpoint contains:

1. The model paths
2. The best models directory
3. The metadata (.meta)
4. The weight indexes (.index)
5. The Losses

* How big is your resulting model checkpoint (mb)?

852.3 MB

* Remember the definition of a "step". How long did an average step take?
An average step took 1.687 sec

* How does that correlate with the observed network utilization between nodes?
The faster the network was, the smaller the step time (in seconds). Thus, we saw a negative correlation between the network speed and the step time.
