# Homework 9: Distributed Training and Neural Machine Translation


## Objective
Using Nvidia [OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq/) framework for sequence to sequence tasks to train a [Transformer-based Machine Translation network](https://nvidia.github.io/OpenSeq2Seq/html/machine-translation/transformer.html) on a small English to German WMT corpus. To do this, we will get a pair of IBM VSI instances with 2 V100 GPUs each.

## Setup

### Creating cloud containers
I creatd two IBM cloud VMs, each with 2 V100 GPUs

```
ibmcloud sl vs create --datacenter=lon04 --hostname=v100a --domain=apik.com --image=2263543 --billing=hourly  --network 1000 --key=<my_key> --flavor AC2_16X120X100 â€“san

ibmcloud sl vs create --datacenter=lon04 --hostname=v100b --domain=apik.com --image=2263543 --billing=hourly  --network 1000 --key=<my_key> --flavor AC2_16X120X100 --san
```
I also added 2 TB of storage on each VM, to make sure they would be able to hold the data and models we would be generating. After starting each of the VMs, I mounted these disks using the following steps:

```
mkdir -m 777 /data
mkfs.ext4 /dev/xvdc
# edit /etc/fstab and all this line: /dev/xvdc /data                   ext4    defaults,noatime        0 0

# Mount
mount /data
```

After creating an account on https://ngc.nvidia.com/ , I logged in on both of my machines and pulled tensorflow

```
docker login nvcr.io
#Username: $oauthtoken
#Password: API KEY
docker pull nvcr.io/nvidia/tensorflow:19.05-py3
```

## Distributed Training
I cloned our repo and build the `openseq2seq1` image and then created a container on both VMs

```
git clone https://github.com/MIDS-scaling-up/v2
cd v2/week09/hw/docker/
docker build -t openseq2seq .

docker run --runtime=nvidia -d --name openseq2seq --net=host -e SSH_PORT=4444 -v /data:/data -p 6006:6006 openseq2seq
```

On each VM, i created an interactive bash session and downloaded the data for training
```
docker exec -ti openseq2seq bash
mpirun -n 2 -H <v100a_private_ip>,<v100b_private_ip>--allow-run-as-root hostname
cd ../opt/OpenSeq2Seq 
scripts/get_en_de.sh /data/wmt16_de_en
cp example_configs/text2text/en-de/transformer-base.py /data
```

Finally, i edited the config file with the following changes:
1. Replace [REPLACE THIS TO THE PATH WITH YOUR WMT DATA] with /data/wmt16_de_en/
1. In base_parms section, replace "logdir": "nmt-small-en-de", with "logdir": "/data/en-de-transformer/", 
1. In eval_params section set "repeat": to True.
1. Modify the config file to use mixed precision per the instructions in the file


I also changed the step limit to 50,000, so as to avoid running out of credits. Once these changes were made, I ran the following on the `v100a` instance:

```
nohup mpirun --allow-run-as-root -n 4 -H <v100a_private_ip>:2,<v100b_private_ip>:2 -bind-to none -map-by slot --mca btl_tcp_if_include eth0 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH python run.py --config_file=/data/transformer-base.py --use_horovod=True --mode=train_eval &
```



## Setbacks while running 
During training, I was unable to start tensorboard from my instance, so the only way I was able to monitor my training was by monitoring the nohup.out file.

Leading up to my succesful training, I had many issues reaching the training stage which took several days. First, I was unable to get two machines with 2 V100 GPUs at the same time. I would start my machines and run `nvidia-smi` each time, but I was constantly faced with the issue where only one of them would have a GPU, or I would get an error running `nvidia-smi`. The eventual solution was to change my machine's location from was04 to lon04

Another issue I had was that I would reach the training stage and and right after running the `nohip mpirun` step, I would receive a message that said `nohup: ignoring input and appending output to nohup.out` I turned out that this could be solved by removing a space I had after the comma separating `<v100a_private_ip>:2,<v100b_private_ip>:2`. Luckily, I was informed via slack by another student of this fix.

## Results

Below are some images from dockerhub. I saved my `/en-de-transformer` directory and copied it to my local machine, from where I was able to start tensorboard to capture these plots:

BLUE Score
![Validation BLEU curve](https://i.ibb.co/g42hJQp/bleu.jpg)

![Validation loss curve](https://i.ibb.co/JdHBvzZ/eval-loss.jpg)

![Learning rate curve](https://i.ibb.co/TmrbKGs/learn-rate.jpg)

![Training loss curve](https://i.ibb.co/T1v9WjT/train-loss.jpg)

![Loss Opt](https://i.ibb.co/0qSzrQY/loss-opt.jpg)

* How long did it take to complete the training run? (hint: this session is on distributed training, so it *will* take a while)
Training took 23 hours and 23 minutes

* Do you think your model is fully trained? How can you tell?
I do not think my model is fully trained, because we can see that the BLEU graph is still progressing and the loss is moving towards, but may not have yet reached a minimum.

* Were you overfitting?
I believe we are beginning to overfit because after 30K steps, we can see that the evaluation loss is flattening out, which leads me to believe that we are overfitting the model.

* Were your GPUs fully utilized?

* Did you monitor network traffic (hint:  ```apt install nmon ```) ? Was network the bottleneck?
* Take a look at the plot of the learning rate and then check the config file.  Can you explan this setting?
* How big was your training set (mb)? How many training lines did it contain?
* What are the files that a TF checkpoint is comprised of?
* How big is your resulting model checkpoint (mb)?
* Remember the definition of a "step". How long did an average step take?
* How does that correlate with the observed network utilization between nodes?

### Hints

Your loss should be something like:


And your learning rate  should be something like:
![Learning rate curve](lr.JPG)
